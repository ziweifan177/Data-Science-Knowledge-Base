#************************
'''
# Last updated: Prabhu Subramanian
# Description: Trying to fix the below - 
               Have introduced 2 parsers
               * nrows(default=100000)
               * max_models(default=10)
'''
#************************

# python3 /home/nik/data_scrubber.py --data='/home/nik/data/titanic_train.csv' --prod='/home/nik/data/titanic_test.csv' --all_variables='/home/nik/data/titanic_variables-1322-1.csv' --target='Survived' --classification=True 
# python3 /home/nik/data_scrubber.py --data='/home/nik/data/ml_airbnb_train.csv' --prod='/home/nik/data/ml_airbnb_prod.csv'  
# python3 /home/nik/data_scrubber.py --data='/home/nik/data/ml_medical_appts_training.csv' --prod='/home/nik/data/ml_medical_appts_prod.csv'  
# python3 /home/nik/data_scrubber.py --data='/home/nik/data/osg_training_demo.csv' --prod='/home/nik/data/osg_prod_demo10k_test.csv'  
# python3 /home/nik/data_scrubber.py --data='/home/nik/data/SQUARK_+Training+-+Leads+&+Campaign+Responses+by+SFDC+Campaign+Name.csv'  --target='Converted'  --all_variables='/home/nik/data/SFDC_variables-1854-1.csv'  --classification=True 
# python3 /home/nik/data_scrubber.py --data='/home/nik/data/updated_clypd_evaluation_data.csv'  --target='RSME_PCT' 
# python3 /home/nik/data_scrubber.py --data='/home/nik/data/Sample_ML_Training_Data_Customer_Churn.csv' --prod='/home/nik/data/Sample_ML_Production_Data_Customer_Churn.csv'  
# python3 /home/nik/data_scrubber.py --rid='scrubber' --data='/home/nik/data/training-file-1852.csv' --prod='/home/nik/data/production-file-1852.csv' --all_variables='/home/nik/data/variables-1852-1.csv' --target='Churn' 
# python3 /home/nik/data_scrubber.py --data='/home/nik/data/Sample_ML_Training_Data_Customer_Churn.csv' --prod='/home/nik/data/Sample_ML_Production_Data_Customer_Churn.csv'  --all_variables='/home/nik/data/variables-1852-1.csv' --target='Churn' 
# python3 /home/nik/data_scrubber.py --data='/home/nik/data/Sample_ML_Training_Data_Customer_Churn.csv' --prod='/home/nik/data/Sample_ML_Production_Data_Customer_Churn.csv'  
# python3 /home/nik/data_scrubber.py --rid='scrubber' --data='/home/nik/data/training-file-1852.csv' --prod='/home/nik/data/production-file-1852.csv' --all_variables='/home/nik/data/variables-1852-1.csv' --target='Churn' 
# python3 /home/nik/data_scrubber.py  --target='Survived' --classification=True  --data='/home/nik/data/titanic_train.csv' --prod='/home/nik/data/titanic_test.csv'  --all_variables='/home/nik/data/titanic_variables.csv'  
# python3 /home/nik/data_scrubber.py  --target='Knee Issue' --classification=True  --data='/home/nik/data/SAMPLE_ML_TRAINING_OR_KNEE_2011_2015_DATA+-+NLC+Mutual.csv' --prod='/home/nik/data/SAMPLE_ML_PRODUCTION_OR_KNEE_2016_DATA+-+NLC+Mutual.csv' 
# python3 /home/nik/data_scrubber.py  --data='/home/nik/data/Seer_FTCGeneralRespModelTraining.csv' --prod='/home/nik/data/Seer_FTCGeneralRespModelProd.csv' 
# ssh nik@35.155.140.237
# scp nik@35.155.140.237:/home/nik/*.csv  ./
# scp nik@35.155.140.237:/home/nik/data/ml_inno_train.csv  ./
# scp *.py nik@35.155.140.237:/home/nik/  
# scp *.csv nik@35.155.140.237:/home/nik/data/ 
'''
sudo pip uninstall h2o
sudo pip3 install -f http://h2o-release.s3.amazonaws.com/h2o/laprod_stable_Py.html h2o
Run "sudo yum update" to apply all updates
sudo pip3 install imbalanced-learn
'''
import h2o
from h2o.automl import H2OAutoML
import random, os, sys
from datetime import datetime
import pandas as pd
import numpy as np
import logging
import csv
import optparse
import time
import json
from distutils.util import strtobool
import psutil

parser=optparse.OptionParser()
parser.add_option('-b', '--bad_levels', help='set bad levels=to false to turn off')
parser.add_option('-c', '--classification', help='classification yes/no')
parser.add_option('-d', '--data', help='data file path')
parser.add_option('-e', '--min_mem_size', help='minimum memory size')
parser.add_option('-f', '--time_factors', help='time factors')
parser.add_option('-g', '--balance', help='balance')  
parser.add_option('-i', '--all_variables', help='all variables')
parser.add_option('-j', '--features', help='features')
parser.add_option('-p', '--path', help='server path')
parser.add_option('-q', '--prod', help='production data')
parser.add_option('-r', '--run_time', help='run time')
parser.add_option('-s', '--scrubber', help='set scrubber to false to turn off')
parser.add_option('-t', '--target', help='dependent variable')
parser.add_option('-x', '--rid', help='run id')
parser.add_option('-n', '--nrows', help='number of rows') # Added for reducing the number of rows for the dataset in case of Big Data
parser.add_option('-m', '--max_models', help='maximum models for aml.train') # Added for limiting the number of models generated by aml.train
(opts, args) = parser.parse_args()



# Functions

def alphabet(n):
  alpha='0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'    
  str=''
  r=len(alpha)-1   
  while len(str)<n:
    i=random.randint(0,r)
    str+=alpha[i]   
  return str
     
def scale_df(df,thresh=100000):
    # df.shape[0]/x=thresh i.e int(thresh/df.shape[0])
    train_split=0.8
    test_split=0.1
    if thresh < df.shape[0]:
      train_split=round(thresh/df.shape[0],1)
      test_split=round((1-train_split)/2,1)   
    train, test, valid = df.split_frame([0.1, 0.9])
    return  train, test, valid 
    
    
def set_meta_data(run_id,server,data,path,prod,min_mem_size):
  m_data={}
  m_data['start_time'] = time.time()
  m_data['server_path']=server
  m_data['data_path']=data 
  m_data['prod_path']=prod
  m_data['run_id'] =run_id
  m_data['end_time'] = time.time()
  m_data['execution_time'] = 0.0
  m_data['run_path'] =path
  m_data['min_mem_size'] = min_mem_size
  return m_data  

def dict_to_json(dct,n):
  j = json.dumps(dct, indent=4)
  f = open(n, 'w')
  print(j, file=f)
  f.close()
  
def get_levels_diff(dat, pro):
    bad_levels={}
    good_levels={}    
    cnt=0    
    ints, reals, enums = [], [], []  
    lst=list(set(dat.columns).intersection(pro.columns))
    pro_types=dict(pro.types.items())      
    for key, val in df.types.items():
      if key in lst:
        if val == 'enum':
          l=[]
          try:
            l=dat[key].levels()[0] 
          except:
            pass            
          p=[]  
          if pro_types[key] == 'enum':
            try:
              p=pro[key].levels()[0] 
            except:
              pass 
          b=list(set(p) - set(l)) 
          if len(b)>0:
            bad_levels[key]=b 
            good_levels[key]=l                 
            cnt+=len(b)
    return bad_levels,good_levels,cnt,lst  

def get_scrubbed_name(p):
  n=''
  (data_root, data_ext) = os.path.splitext(p)
  data_names=data_root.split('/')
  n=data_names[-1]+'_scrubbed'+data_ext
  return n  
  
    
def get_independent_variables(df, targ):
    C = [name for name in df.columns if name != targ]
    # determine column types
    ints, reals, enums = [], [], []
    for key, val in df.types.items():
        if key in C:
            if val == 'enum':
                enums.append(key)
            elif val == 'int':
                ints.append(key)            
            else: 
                reals.append(key)    
    x=ints+enums+reals
    return x
    
    
def get_all_variables_csv(i):
    ivd={}
    try:
      iv = pd.read_csv(i,header=None)
    except:
      logging.critical('pd.read_csv get_all_variables') 
      h2o.download_all_logs(dirname=logs_path, filename=logfile)
      h2o.cluster().shutdown()     
      sys.exit(9)             
    col=iv.values.tolist()[0]
    dt=iv.values.tolist()[1]
    i=0
    for c in col:
      ivd[c.strip()]=dt[i].strip()
      i+=1        
    return ivd

    

def check_all_variables(df,dct,y=None):     
    targ=list(dct.keys())     
    for key, val in df.types.items():
        if key in targ:
          if dct[key] not in ['real','int','enum']:                      
            targ.remove(key)  
    for key, val in df.types.items():
        if key in targ:            
          if dct[key] != val:
            if dct[key]=='enum':
                try:
                  df[key] = df[key].asfactor() 
                except:
                  targ.remove(key)                 
            if dct[key]=='int': 
                try:                
                  df[key] = df[key].asnumeric() 
                except:
                  targ.remove(key)                  
            if dct[key]=='real':
                try:                
                  df[key] = df[key].asnumeric()  
                except:
                  targ.remove(key)                  
    if y is None:
      y=df.columns[-1] 
    if y in targ:
      targ.remove(y)           
    return targ     

def get_model_by_algo(algo,models_dict):
    mod=None
    mod_id=None    
    for m in list(models_dict.keys()):
        if m[0:3]==algo:
            mod_id=m
            mod=h2o.get_model(m)
            return mod,mod_id
    return mod,mod_id 

def feature_selection(d,l, thresh=0.05):
    iv=[]
    for key in d.keys():
      if d[key]>thresh:
        iv.append(key)
    for v in iv:
        if v not in l:
            iv.remove(v)
    return iv

def split_date_time_to_columns(df):
    try:
        for key,value in df.types.items():
            if(value=='time'):
                df[key + '_day'] = df[key].day()
                df[key + '_month'] = df[key].month()
                df[key + '_week'] = df[key].week()
                df[key + '_year'] = df[key].year()
                df[key + '_dayOfWeek'] = df[key].dayOfWeek()
                df[key + '_hour'] = df[key].hour()
                df[key + '_minute'] = df[key].minute()
                df[key + '_second'] = df[key].second()
        return df
    except:
        pass

#Calculating Ratio of the imbalanced dataset
def find_imbalanced_ratio(data, target, threshold=0.4): 
    maxPercentage = (data[target].value_counts().max()/data[target].count())*100
    minPercentage = (data[target].value_counts().min()/data[target].count())*100
    if minPercentage < threshold * 100 :
        print("Dataset is Imbalanced")
    else:
        print("Dataset is Balanced")
    return maxPercentage, minPercentage

def make_over_samples_SMOTE(data,target, threshold=0.4):
 #input DataFrame
 #X →Independent Variable in DataFrame\
 #y →dependent Variable in Pandas DataFrame format
    Xt = data.iloc[:, data.columns != target]
    yt = data.iloc[:, data.columns == target]
    from imblearn.over_sampling import SMOTE
    sm = SMOTE(ratio = threshold)
    Xt, yt = sm.fit_sample(Xt, yt)
    return Xt,yt    


def append_new_variables(df,dct,y=None):     
    targ=list(dct.keys())      
    for key, val in df.types.items():
        if key not in targ:   
          dct[key]=val        
    return dct         
 
def get_all_variables_csv_ivd(i,y=None):
    ivd={}
    Xv=[]    
    try:
      iv = pd.read_csv(i,header=None)
    except:
      logging.critical('pd.read_csv get_all_variables') 
      h2o.download_all_logs(dirname=logs_path, filename=logfile)
      h2o.cluster().shutdown()     
      sys.exit(9)   
    col=iv.values.tolist()[0]
    dt=iv.values.tolist()[1]
    i=0
    for c in col:
      ivd[c.strip()]=dt[i].strip()
      i+=1 
    Xv=list(ivd.keys())
    if y in Xv:
      Xv.remove(y)    
    return ivd,Xv    
    
def get_all_variables(df):
    V = {}
    for key, val in df.types.items():
      V[key]=val
    return V
    
def check_Xy(l,df):
  sub=list(set(df.columns) & set(l)) 
  dif=list(set(l)-set(sub))
  return sub,dif    
        
def all_variables_Xy(df_pd,l):
  c=[]
  i=0
  dl=list(df_pd.columns)
  slice=list(set(dl) & set(l))
  df_pd = df_pd[slice] 
  return df_pd
  
      
#  End Functions

# 90% of memory limitation 

min_mem_size=4
pct_memory=0.9
virtual_memory=psutil.virtual_memory()
max_mem_size=int(round(int(pct_memory*virtual_memory.available)/1073741824,0))
if min_mem_size > max_mem_size:
  min_mem_size=max_mem_size-1


# Settings

balance_flag=False
min_Xy_len=3


#if True:

try:

# parser.add_option('-d', '--data', help='data file path')
     
    data_path=None
    if opts.data is not None:
      try:
        data_path=str(opts.data)  # data_path = '/Users/bear/Downloads/H2O/VD/data/loan.csv'
      except:
        sys.exit(5) 
     
    
    # parser.add_option('-q', '--prod', help='production data')
    
    prod_path=None
         
    if opts.prod is not None:
      try:
        prod_path=str(opts.prod)  # prod = '/Users/bear/Downloads/H2O/VD/data/loan.prod.csv'
      except:
        sys.exit(5)   
     
    
    # parser.add_option('-x', '--rid', help='run id')   
      
    rid=None
         
    if opts.rid is not None:
      try:
        rid=str(opts.rid)  # prod = '/Users/bear/Downloads/H2O/VD/data/loan.prod.csv'
      except:
        sys.exit(5) 
        
    n_rows=100000
    if opts.nrows is not None:
      try:
        n_rows=int(opts.nrows)
      except:
        sys.exit(5)  
      
    max_models=10
    if opts.max_models is not None:
      try:
        max_models=int(opts.max_models)
      except:
        sys.exit(5)  
            
    # parser.add_option('-e', '--min_mem_size', help='minimum memory size')
        
    # min_mem_size=int(round(int(pct_memory*virtual_memory.available)/1073741824,0))
    
    if opts.min_mem_size is not None:
      try:
        min_mem_size=int(opts.min_mem_size)  
      except:
        sys.exit(5)     
      
    classification=False
      
    if opts.classification is not None:
      try:
        classification=bool(strtobool(opts.classification))  # classification=True
      except:
        sys.exit(5)     
      
    bad_levels=True  # Set bad_levels=False to production level remover    
    if opts.bad_levels is not None:
      try:
        bad_levels=bool(strtobool(opts.bad_levels))  
      except:
        sys.exit(5)  
        
        
    # run_time=360       
    run_time=222
    
    if opts.run_time is not None:
      try:
        run_time=int(opts.run_time)  # run_time=360
      except:
        sys.exit(5)      
      
    features=True # Set time_factors=False to turn off time_factors    
      
       
    if opts.features is not None:
      try:
        features=bool(strtobool(opts.features))  
      except:
        sys.exit(5)      
        
        
    time_factors=True # Set time_factors=False to turn off time_factors    
      
       
    if opts.time_factors is not None:
      try:
        time_factors=bool(strtobool(opts.time_factors))  
      except:
        sys.exit(5)  
                    
    balance=True # Set balance=False to turn off balance    
                 
    if opts.balance is not None:
      try:
        balance=bool(strtobool(opts.balance))  
      except:
        sys.exit(5)   
                          
    scrubber=True # Set scrubber=False to turn off scrubber     
     
    # scrubber=False # Set scrubber=False to turn off scrubber   
       
    if opts.scrubber is not None:
      try:
        scrubber=bool(strtobool(opts.scrubber))  
      except:
        sys.exit(5)  
                    
        
    server_path=None
    
    if opts.path is not None:
      try:
        server_path=str(opts.path)  # server_path='/Users/bear/Downloads/H2O/VD'
      except:
        sys.exit(5) 
        
    all_variables=None
    if opts.all_variables is not None:
      try:
        all_variables=str(opts.all_variables)  # data_path = '/Users/bear/Downloads/H2O/VD/data/loan.fields.csv'
      except:
        sys.exit(5) 
        
    target=None
         
    if opts.target is not None:
      try:
        target=str(opts.target)  # target = 'dependent_variable'
      except:
        sys.exit(5)            
        
        
    # Start
        
    start = datetime.now()
      
    if rid is None:      
      run_id=alphabet(9)
    else:      
      run_id=rid   
    if server_path==None:
      server_path=os.path.abspath(os.curdir)
    os.chdir(server_path) 
    run_dir = os.path.join(server_path,run_id)
    os.mkdir(run_dir)
    os.chdir(run_dir)    
    
    print (run_id) # run_id to std out
    
    
    # Logs
    
    logfile=run_id+'_autoh2o_log.zip'
    logs_path=os.path.join(run_dir,'logs')
    
    
    # logging
    log_file=run_id+'.log'
    log_file = os.path.join(run_dir,log_file)
    logging.basicConfig(filename=log_file,level=logging.INFO,format="%(asctime)s:%(levelname)s:%(message)s")
    logging.info(start) 
    
    # 65535 Highest port no
    port_no=random.randint(5555,55555)
    
    #  h2o.init(strict_version_check=False,min_mem_size_GB=min_mem_size,port=port_no) # start h2o
    try:
      h2o.init(strict_version_check=False,min_mem_size_GB=min_mem_size,max_mem_size_GB=max_mem_size,port=port_no,nthreads=-1) # start h2o
    except:
      logging.critical('h2o.init')
      h2o.download_all_logs(dirname=logs_path, filename=logfile)      
      h2o.cluster().shutdown()
      sys.exit(2)
    
    
    
    scrubber=True # Set scrubber=False to turn off scrubber
    bad_levels=True  # Set bad_levels=False to production level remover
    
    # meta data
    meta_data = set_meta_data(run_id,server_path,data_path,prod_path,run_dir,min_mem_size)
    
    # Start of app
    try:
        df = h2o.import_file(data_path,header=1) # Added header=1 to make sure the dataset is read with the firstline being its header
         # Changes
        print(df.shape)# Changes
         #     df, test, valid = scale_df(df)# Changes
        df = df[:n_rows,:]# Changes # Using the nrows - reducing the number of rows
        print(df.shape)# Changes # viewing the reduced number of rows
      
    except:
      logging.critical('h2o.import_file(data_path)')    
      h2o.download_all_logs(dirname=logs_path, filename=logfile)    
      h2o.cluster().shutdown()
      sys.exit(3)
    
    # Load and analyze production versus training files
    
    data_orig=df
    
    prod=None  
    
    if prod_path is not None:
      try:  
        prod = h2o.import_file(prod_path,header=1) 
    # Changes
        print(prod.shape)# Changes 
    #       prod, test, valid = scale_df(prod)# Changes
        prod = prod[:n_rows,:]# Changes Using the nrows - reducing the number of rows
        print(prod.shape)# Changes # viewing the reduced number of rows
      except:
        logging.critical('h2o.import_file(prod_path)')    
        h2o.download_all_logs(dirname=logs_path, filename=logfile)    
        h2o.cluster().shutdown()
        sys.exit(3)
    
    if prod is not None:
      prod_orig=prod  
    
    # independent variables
    X = []  
    ivd = {}
    
    if target is None:  
      target=df.columns[-1]   
        
    orig_X=get_independent_variables(df, target)
    meta_data['orig_X']=orig_X  
    
    if all_variables is None:
      X=get_independent_variables(df, target)  
    else: 
      ivd,X=get_all_variables_csv_ivd(all_variables, target)   
    
    meta_data['ivd']=ivd
    
    Xy=[]      
    for e in X:
      Xy.append(e)
    
    if target is not None:
      Xy.append(target)
        
    # Set balance=False to turn off balance   
    if (balance is True) and (target is not None):
      pass
    
    
    # Set time_factors=False to turn off time factors
    if (time_factors is True):
      df=split_date_time_to_columns(df)
      if prod is not None:
        prod=split_date_time_to_columns(prod)    
    
    
    
    # Start feature selection
    alg_start_time = time.time()
    
    # Set up AutoML
    # Changes introducing max_models=9  
    aml = H2OAutoML(max_runtime_secs=run_time, max_models=max_models)# Changes # Limiting the max_models to 10(Default if user hasn't given any input)
    
    if (features is True) and (target is not None):
      try:
        no_rows=round(df.shape[0]*0.9,0)  # 90% of rows
       
        if no_rows> 100000:  # If 90% > 100000 limit to 100000 
          no_rows=100000
      
      # Cut data to roughly no_rows if data have more rows than no_rows
      
        pct_rows=round(no_rows/df.shape[0],2)
        if pct_rows < 0.02:
          pct_rows=0.02   
          
        train, test, valid = [[],[],[]]
        half_pct=0.0
        if pct_rows < 1:
          half_pct=round(((1-pct_rows)/2),2)  
          train, test, valid = df.split_frame([pct_rows,half_pct])
        
        if target==None:
          target=train.columns[-1] 
          
        y = target
      
        X = []  
        if all_variables is None:
          X=get_independent_variables(train, target)  
        else: 
          ivd=get_all_variables_csv(all_variables)    
          X=check_all_variables(train,ivd,y)
          
        if classification:
          train[y] =  train[y].asfactor()
            
        try:
          aml.train(x=X,y=y,training_frame=train)  # Change training_frame=train
        except Exception as e:
          logging.critical('aml.train') 
          h2o.download_all_logs(dirname=logs_path, filename=logfile)      
          h2o.cluster().shutdown()   
          sys.exit(4)    
         
        lb = aml.leaderboard
        aml_leaderboard_df=aml.leaderboard.as_data_frame()
        
        models_dict={}
        for m in aml_leaderboard_df['model_id']:
           models_dict[m]=None 
        
        varimp={}    
          
        mod,mod_id=get_model_by_algo("DRF",models_dict)
      
        if mod is not None:
          l=mod.varimp()
          for v in l:
            varimp[v[0]]=v[2]
          
        mod,mod_id=get_model_by_algo("XRT",models_dict)
        if mod is not None:    
          l=mod.varimp()
          for v in l:
            if v[0] in varimp:   
              varimp[v[0]]=((v[2]+varimp[v[0]])/2)
            else:   
              varimp[v[0]]=((v[2]+0.0)/2) 
              
        mod,mod_id=get_model_by_algo("GBM",models_dict)
        if mod is not None:    
          l=mod.varimp()
          for v in l:
            if v[0] in varimp:   
              varimp[v[0]]=((v[2]+varimp[v[0]])/2)
            else:   
              varimp[v[0]]=((v[2]+0.0)/2)         
               
        X=feature_selection(varimp,X)   
        Xy=[y]    
        for e in X:
          Xy.append(e)
    
        meta_data['varimp']=varimp 
        if len(Xy) > min_Xy_len:
          df=df[:,Xy]     
        else:
          X=get_independent_variables(train, target) 
          Xy=[y]    
          for e in X:
            Xy.append(e)
        meta_data['Xy']=Xy  
        meta_data['X']=X  
      
      except:
        logging.warning('h2o.feature_selection')  
    meta_data['feature_selection_execution_time'] = time.time() - alg_start_time     
    # End feature selection    
    
    # Balance
      
    df_pd=None
    
    # Check bad levels in production
    
    bad_lev,good_lev,diff={},{},{}
    if (df is not None) and (prod is not None):
      bad_lev,good_lev,diff,common=get_levels_diff(df, prod)
    
    
    # Update meta_data
    
    meta_data['num_bad_levels']=len(bad_lev.keys())
    meta_data['bad_levels']=bad_lev
    meta_data['num_bad_levels']=len(bad_lev.keys())  
    before_after=[]
    if (prod is not None):
      before_after=[prod.isna().sum(),0]
    
    
    # 'NaN'
    
    nvalue = float('NaN')
    
    
    # Replace bad levels in production with empty string
    
    if (prod is not None) and (len(bad_lev.keys())>0) and (bad_levels is True):
      for lev in bad_lev:
        prod[lev] = (prod[lev].isin(bad_lev[lev])).ifelse('', prod[lev])
    
    
    # Update meta_data
    
    if (prod is not None):
      before_after[1]=prod.isna().sum()
    
    meta_data['before_after']=before_after
    
    
    # Export training scrubbed
    no_parts=-1
    
    
    cur_path=os.path.abspath(os.curdir)
    n=get_scrubbed_name(data_path)
    out_path=os.path.join(cur_path,n)
    meta_data['training_scrubbed_name'] = n
    
    if train is None:
      train, test = df.split_frame([0.05])
      if target==None:
        target=train.columns[-1] 
      if classification:
        train[y] =  train[y].asfactor()
            
    
    # check_Xy
    
    Xy,d=check_Xy(Xy,df)
    
    
    if len(d)>0:
      df=df[:,Xy]  
    
    try:
        if (scrubber is True):
          df_pd=df.as_data_frame() 
          df_pd.to_csv(out_path, index=False)  # Scrubbed file
        else:
          data_orig=data_orig.as_data_frame()    
          data_orig.to_csv(out_path, index=False)  # Scrubbed file
    except:
      logging.critical('data.scrubber.issue') 
      h2o.download_all_logs(dirname=logs_path, filename=logfile)      
      h2o.cluster().shutdown()   
      sys.exit(11)
    '''    
    if (scrubber is True):
      h2o.export_file(df, out_path)              # Scrubbed file
    else:
      h2o.export_file(data_orig,out_path) 
    '''  
    
    
    # Export production scrubbed
    try:
        if (prod is not None):
          X,d=check_Xy(X,prod)    
        #  if len(d)>0: 
          if True:    
            prod=prod[:,X]
          n=get_scrubbed_name(prod_path)
          out_path=os.path.join(cur_path,n)
          meta_data['production_scrubbed_name'] = n
          if (scrubber is True):
            prod_pd=prod.as_data_frame() 
            prod_pd.to_csv(out_path, index=False)  # Scrubbed file
          else:
            prod_pd=prod_orig.as_data_frame() 
            prod_pd.to_csv(out_path, index=False)  # Scrubbed file   
    except:
        logging.critical('prod.scrubber.issue') 
        h2o.download_all_logs(dirname=logs_path, filename=logfile)      
        h2o.cluster().shutdown()   
        sys.exit(12)
    
    '''    
    if (prod is not None):
      X,d=check_Xy(X,prod)    
    #  if len(d)>0:  
      if True:    
        prod=prod[:,X]
      n=get_scrubbed_name(prod_path)
      out_path=os.path.join(cur_path,n)
      meta_data['production_scrubbed_name'] = n
      if (scrubber is True):
        h2o.export_file(prod, out_path)              # Scrubbed file
      else:
        h2o.export_file(prod_orig, out_path)     
    '''      
    # Scrubbed file    
    
    # Write variables scrubbed
    if all_variables is not None:
      all_variables_name=os.path.basename(all_variables)
      (f,ext)=os.path.splitext(all_variables_name) 
      all_variables_name=f+'_scrubbed'+ext  
      ivd=get_all_variables_csv(all_variables)    
      new_var=append_new_variables(df,ivd)  
      var_df = pd.DataFrame.from_dict(new_var, orient="index")
      var_df = var_df.transpose()          
      var_df = all_variables_Xy(var_df,Xy)    
      var_df.to_csv(all_variables_name, index=False)   
      
    # End of app
     
    # Update and save meta data
    
    meta_data['Xy']=Xy  
    meta_data['X']=X  
      
    meta_data['end_time'] = time.time()
    meta_data['execution_time'] = meta_data['end_time'] - meta_data['start_time']
      
    n=run_id+'_meta_data.json'
    dict_to_json(meta_data,n)    
    
    
    # Save logs
#     h2o.download_all_logs(dirname=logs_path, filename=logfile)
    os.chdir(server_path)
    
    h2o.cluster().shutdown()
#     sys.exit(0)
     
except Exception as e:
# else:
  logging.critical('h2o.unknown')
  h2o.download_all_logs(dirname=logs_path, filename=logfile)   
  h2o.cluster().shutdown()
  sys.exit(9)
  
  

